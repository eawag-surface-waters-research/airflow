{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c60510b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import pytz\n",
    "import requests\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbbe82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"https://alplakes-eawag.s3.eu-central-1.amazonaws.com\"\n",
    "bucket_key = bucket.split(\".\")[0].split(\"//\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d2de78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class boto3(object):\n",
    "    def __init__(self):\n",
    "        self.intialise = \"\"\n",
    "    def upload_file(self, file, bucket, path):\n",
    "        print(file, bucket, path)\n",
    "\n",
    "s3 = boto3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fffc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake = \"geneva\"\n",
    "model = \"delft3d-flow\"\n",
    "api = \"https://alplakes-api.eawag.ch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84b44f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpxvxde8pz alplakes-eawag simulations/delft3d-flow/cache/geneva/performance.json\n"
     ]
    }
   ],
   "source": [
    "# Collect model metadata\n",
    "response = requests.get(\"{}/simulations/metadata/{}/{}\".format(api, model, lake))\n",
    "if response.status_code != 200:\n",
    "    raise ValueError(\"Unable to access {}/simulations/metadata/{}/{}\".format(api, model, lake))\n",
    "lake_metadata = response.json()\n",
    "\n",
    "\n",
    "# Get version of website metadata\n",
    "branch = \"master\"\n",
    "try:\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/eawag-surface-waters-research/alplakes-react/refs/heads/master/src/config.json\")\n",
    "    if response.status_code == 200:\n",
    "        branch = response.json()[\"branch\"]\n",
    "except:\n",
    "    print(\"Failed to find branch\")\n",
    "\n",
    "default_depth = 1\n",
    "default_period = -6\n",
    "response = requests.get(\"{}/static/website/metadata/{}/{}.json\".format(bucket, branch, lake))\n",
    "if response.status_code != 200:\n",
    "    raise ValueError(\"Unable to access {}/static/website/metadata/{}/{}.json\".format(bucket, branch, lake))\n",
    "lake_info = response.json()\n",
    "try:\n",
    "    default_depth = lake_info[\"properties\"][\"default_depth\"]\n",
    "except:\n",
    "    print(\"Failed to collect custom depth, using default of {}\".format(default_depth))\n",
    "    \n",
    "# Performance\n",
    "try:\n",
    "    if \"live\" in lake_info[\"forecast\"][\"3d_model\"][\"performance\"]:\n",
    "        live = lake_info[\"forecast\"][\"3d_model\"][\"performance\"][\"live\"].copy()\n",
    "        stop = datetime.now() - timedelta(days=1)\n",
    "        stop = stop.replace(hour=23, minute=0, second=0, microsecond=0)\n",
    "        start = stop - timedelta(days=10)\n",
    "        rmse_total = []\n",
    "        for location in live:\n",
    "            for depth in live[location][\"depth\"]:\n",
    "                try:\n",
    "                    if live[location][\"type\"] == \"datalakes\":\n",
    "                        live[location][\"depth\"][depth][\"insitu\"] = download_datalakes_data(live[location][\"id\"], live[location][\"depth\"][depth][\"depth\"], start, stop)\n",
    "                    else:\n",
    "                        raise ValueError(\"Unrecognised data source\")\n",
    "                    response = requests.get(\"{}/simulations/point/{}/{}/{}/{}/{}/{}/{}?variables=temperature\".format(api, model, lake, start.strftime(\"%Y%m%d2300\"), stop.strftime(\"%Y%m%d2300\"), live[location][\"depth\"][depth][\"depth\"], live[location][\"lat\"], live[location][\"lng\"]))    \n",
    "                    if response.status_code != 200:\n",
    "                        raise ValueError(\"Failed to get model values\")\n",
    "                    out = response.json()\n",
    "                    live[location][\"depth\"][depth][\"model\"] = {\"time\": out[\"time\"], \"values\": out[\"variables\"][\"temperature\"][\"data\"]}\n",
    "                    rmse = calculate_rmse(live[location][\"depth\"][depth][\"model\"], live[location][\"depth\"][depth][\"insitu\"])\n",
    "                    if isinstance(rmse, float) and not math.isnan(rmse):\n",
    "                        rmse_total.append(rmse)\n",
    "                        live[location][\"depth\"][depth][\"rmse\"] = round(rmse, 1)\n",
    "                except:\n",
    "                    print(\"Failed to collect insitu\")\n",
    "        if len(rmse_total) > 0:\n",
    "            lake_metadata[\"rmse\"] = round(np.nanmean(np.array(rmse_total)), 1)\n",
    "            with open(\"performance.json\",mode='w') as temp_file:\n",
    "                json.dump(live, temp_file)\n",
    "            with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_file:\n",
    "                temp_filename = temp_file.name\n",
    "                json.dump(live, temp_file)\n",
    "            s3.upload_file(temp_filename, bucket_key, \"simulations/{}/cache/{}/performance.json\".format(model, lake))\n",
    "except:\n",
    "    print(\"Live performance failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d46cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_datalakes_data(datalakes_id, depth, start, stop):\n",
    "    response = requests.get(\"https://api.datalakes-eawag.ch/datasetparameters?datasets_id={}\".format(datalakes_id))    \n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(\"Failed to get datalakes parameters\")\n",
    "    parameters = response.json()\n",
    "    time_axis = [p for p in parameters if p.get(\"parameters_id\") == 1][0][\"axis\"]\n",
    "    depth_axis = [p for p in parameters if p.get(\"parameters_id\") == 2][0][\"axis\"]\n",
    "    value_axis = [p for p in parameters if p.get(\"parameters_id\") == 5][0][\"axis\"]\n",
    "    \n",
    "    response = requests.get(\"https://api.datalakes-eawag.ch/files?datasets_id={}\".format(datalakes_id))    \n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(\"Failed to get datalakes files\")\n",
    "    files = response.json()\n",
    "    files = [f for f in files if f[\"filetype\"] == \"json\"]\n",
    "    files = sorted(files, key=lambda x: datetime.strptime(x[\"maxdatetime\"], \"%Y-%m-%dT%H:%M:%S.%fZ\"))    \n",
    "    file_ids = [files[-1][\"id\"]]\n",
    "    if datetime.strptime(files[-1][\"mindatetime\"], \"%Y-%m-%dT%H:%M:%S.%fZ\") > start:\n",
    "        file_ids.insert(0, files[-2][\"id\"])\n",
    "    time = []\n",
    "    values = []\n",
    "    \n",
    "    for file_id in file_ids:\n",
    "        response = requests.get(\"https://api.datalakes-eawag.ch/files/{}?get=raw\".format(file_id))    \n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(\"Failed to get datalakes files\")\n",
    "        data = response.json()\n",
    "        t = [datetime.fromtimestamp(d) for d in data[time_axis]]\n",
    "        d_idx = min(range(len(data[depth_axis])), key=lambda i: abs(data[depth_axis][i] - depth))\n",
    "        v = np.array(data[value_axis])[d_idx, :]\n",
    "        time = time + t\n",
    "        values = values + v.tolist()\n",
    "    \n",
    "    df = pd.DataFrame({'time': time,'value': values})\n",
    "    df = df.dropna()\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df = df.sort_values(by='time')\n",
    "    df = df[(df['time'] >= start) & (df['time'] <= stop)]\n",
    "    if len(df) < 10:\n",
    "        raise ValueError(\"Not enough data to compare\")\n",
    "    time = df['time'].dt.strftime('%Y-%m-%dT%H:%M:%S+00:00').tolist()\n",
    "    values = df['value'].tolist()   \n",
    "    return {\"time\": time, \"values\": values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c09336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(dataset1, dataset2):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Square Error (RMSE) between two datasets,\n",
    "    where one dataset has timestamps every 3 hours and the other has timestamps every 10 minutes.\n",
    "    Only consider a 10-minute timestamp if it is within 20 minutes of the 3-hour timestamp.\n",
    "\n",
    "    Args:\n",
    "        dataset1 (dict): A dictionary with 'time' (every 3 hours) and 'values' lists for the first dataset.\n",
    "        dataset2 (dict): A dictionary with 'time' (every 10 minutes) and 'values' lists for the second dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The RMSE value.\n",
    "    \"\"\"\n",
    "    # Convert time strings to datetime objects for easier manipulation\n",
    "    time1 = [datetime.fromisoformat(t) for t in dataset1['time']]\n",
    "    time2 = [datetime.fromisoformat(t) for t in dataset2['time']]\n",
    "\n",
    "    # Time threshold for matching (20 minutes)\n",
    "    time_threshold = timedelta(minutes=20)\n",
    "\n",
    "    # For each time in dataset1 (3-hour points), find the closest time in dataset2 (10-min points)\n",
    "    aligned_values1 = []\n",
    "    aligned_values2 = []\n",
    "    \n",
    "    for t1 in time1:\n",
    "        # Find the closest time in dataset2 that is within 20 minutes of t1\n",
    "        valid_times = [t2 for t2 in time2 if abs(t2 - t1) <= time_threshold]\n",
    "        \n",
    "        if valid_times:\n",
    "            # Find the closest time from the valid options\n",
    "            closest_time = min(valid_times, key=lambda t2: abs(t2 - t1))\n",
    "            # Get the index of the closest time in dataset2\n",
    "            index2 = time2.index(closest_time)\n",
    "            \n",
    "            # Append the corresponding values to the aligned lists\n",
    "            aligned_values1.append(float(dataset1['values'][time1.index(t1)]))  # Ensure it's a float\n",
    "            aligned_values2.append(float(dataset2['values'][index2]))  # Ensure it's a float\n",
    "\n",
    "    # Ensure both lists of aligned values have the same length\n",
    "    if len(aligned_values1) != len(aligned_values2):\n",
    "        raise ValueError(\"The datasets do not align properly in time or length.\")\n",
    "\n",
    "    # Compute the squared differences between corresponding values\n",
    "    squared_differences = [(v1 - v2)**2 for v1, v2 in zip(aligned_values1, aligned_values2)]\n",
    "\n",
    "    # Calculate RMSE\n",
    "    mse = np.mean(squared_differences)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e88b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af74272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "airflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
